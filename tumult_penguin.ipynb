{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9838f37-ea8c-4c8c-be02-0ec9bdce9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tmlt.core\n",
    "!pip install tmlt.analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694331a-bbb4-42c5-b2eb-d25c2a601515",
   "metadata": {},
   "source": [
    "# Explore Tumult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66cc51-5cd5-4e9e-9f07-2745312bb0fe",
   "metadata": {},
   "source": [
    "From doc, tumult with python 3.9 to 3.11. \n",
    "\n",
    "Onyxia 3.13 spark not working with Tumult. 3.12 seems ok for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fa31db-5c28-4194-b534-e74eff00adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmlt.analytics.utils import check_installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4160f46-ff9f-4bb3-94eb-9db9b7ef53e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tumult Core 0.18.2\n",
      "Using Tumult Analytics 0.20.2\n",
      "Creating Spark session... Using PySpark 3.5.7\n",
      " OK\n",
      "Creating Pandas dataframe...  OK\n",
      "Converting to Spark dataframe...  OK\n",
      "Creating Tumult Analytics session...  OK\n",
      "Creating query... OK\n",
      "Evaluating query..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:31:56.682 [task-result-getter-1] ERROR org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 17.0 failed 4 times; aborting job\n",
      " FAILED\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n\nThe installation test did not complete successfully. You may want to\ncheck:\n- your Java installation (try `java -version`)\n- your PySpark and Pandas installations (run `pip3 show pyspark pandas`)\n\nFor more information, see the Tumult Analytics installation instructions\nat\nhttps://docs.tmlt.dev/analytics/latest/howto-guides/installation.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/analytics/utils.py:110\u001b[39m, in \u001b[36mcheck_installation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating query...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m result = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_expr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivacy_budget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPureDPBudget\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m OK\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/analytics/session.py:1148\u001b[39m, in \u001b[36mSession.evaluate\u001b[39m\u001b[34m(self, query_expr, privacy_budget)\u001b[39m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accountant\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43madjusted_budget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InsufficientBudgetError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:1327\u001b[39m, in \u001b[36mPrivacyAccountant.measure\u001b[39m\u001b[34m(self, measurement, d_out)\u001b[39m\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28mself\u001b[39m._privacy_budget = \u001b[38;5;28mself\u001b[39m._privacy_budget.subtract(d_out)\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_queryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMeasurementQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMakeInteractive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_out\u001b[49m\u001b[43m=\u001b[49m\u001b[43md_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:291\u001b[39m, in \u001b[36mSequentialQueryable.__call__\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m._remaining_budget = \u001b[38;5;28mself\u001b[39m._remaining_budget.subtract(privacy_loss)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m retirable_answer = RetirableQueryable(\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m._previous_queryable = retirable_answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:766\u001b[39m, in \u001b[36mMakeInteractive.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns a :class:`~.GetAnswerQueryable`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGetAnswerQueryable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_measurement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:371\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, measurement, data)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMeasurement must be non-interactive.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28mself\u001b[39m._answer = \u001b[43mmeasurement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/chaining.py:142\u001b[39m, in \u001b[36mChainTM.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes measurement after applying transformation on input data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_measurement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/postprocess.py:189\u001b[39m, in \u001b[36mNonInteractivePostProcess.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# TODO(#1176): Retire the queryable after calling self.f.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/analytics/_query_expr_compiler/_base_measurement_visitor.py:468\u001b[39m, in \u001b[36mBaseMeasurementVisitor._build_adaptive_groupby_agg_and_noise_info.<locals>.perform_groupby_agg\u001b[39m\u001b[34m(queryable)\u001b[39m\n\u001b[32m    467\u001b[39m agg = build_groupby_agg_from_groupby(groupby)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mqueryable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMeasurementQuery\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:410\u001b[39m, in \u001b[36mDecoratedQueryable.__call__\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Answers query.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._postprocess_answer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_queryable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:291\u001b[39m, in \u001b[36mSequentialQueryable.__call__\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m._remaining_budget = \u001b[38;5;28mself\u001b[39m._remaining_budget.subtract(privacy_loss)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m retirable_answer = RetirableQueryable(\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasurement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m._previous_queryable = retirable_answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:766\u001b[39m, in \u001b[36mMakeInteractive.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns a :class:`~.GetAnswerQueryable`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGetAnswerQueryable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_measurement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/interactive_measurements.py:371\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, measurement, data)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMeasurement must be non-interactive.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28mself\u001b[39m._answer = \u001b[43mmeasurement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/chaining.py:142\u001b[39m, in \u001b[36mChainTM.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes measurement after applying transformation on input data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_measurement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/spark_measurements.py:76\u001b[39m, in \u001b[36mSparkMeasurement.__call__\u001b[39m\u001b[34m(self, val)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs measurement and returns a DataFrame with additional protections.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03mSee :ref:`pseudo-side-channel-mitigations` for more details on the specific\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03mmitigations we apply here.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_sanitized_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/measurements/spark_measurements.py:888\u001b[39m, in \u001b[36m_get_sanitized_df\u001b[39m\u001b[34m(sdf)\u001b[39m\n\u001b[32m    884\u001b[39m \u001b[38;5;66;03m# repartitioning by a column of random numbers ensures that the content\u001b[39;00m\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# of partitions of the output DataFrame is determined randomly.\u001b[39;00m\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# for each row, its partition number (the partition index that the row is\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;66;03m# distributed to) is determined as: `hash(partitioning_column) % num_partitions`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_materialized_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartitioning_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartitioning_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartitioning_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43msortWithinPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtable_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muuid\u001b[49m\u001b[43m.\u001b[49m\u001b[43muuid4\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhex\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/core/utils/misc.py:105\u001b[39m, in \u001b[36mget_materialized_df\u001b[39m\u001b[34m(sdf, table_name)\u001b[39m\n\u001b[32m    104\u001b[39m spark.catalog.setCurrentDatabase(Config.temp_db_name())\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43msdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m materialized_df = spark.read.table(table_name).toDF(*col_names)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/spark/python/pyspark/sql/readwriter.py:1586\u001b[39m, in \u001b[36mDataFrameWriter.saveAsTable\u001b[39m\u001b[34m(self, name, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1585\u001b[39m     \u001b[38;5;28mself\u001b[39m.format(\u001b[38;5;28mformat\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPythonException\u001b[39m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'tmlt'\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcheck_installation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.12/site-packages/tmlt/analytics/utils.py:159\u001b[39m, in \u001b[36mcheck_installation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m FAILED\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e).startswith(\u001b[33m\"\u001b[39m\u001b[33mIt looks like the analytics session\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    160\u001b[39m         dedent(\n\u001b[32m    161\u001b[39m \u001b[38;5;250m            \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03m        The installation test did not complete successfully. You may want to\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m        check:\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m        - your Java installation (try `java -version`)\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m        - your PySpark and Pandas installations (run `pip3 show pyspark pandas`)\u001b[39;00m\n\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m \u001b[33;03m        For more information, see the Tumult Analytics installation instructions\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m        at\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03m        https://docs.tmlt.dev/analytics/latest/howto-guides/installation.html\"\"\"\u001b[39;00m\n\u001b[32m    171\u001b[39m         )\n\u001b[32m    172\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: \n\nThe installation test did not complete successfully. You may want to\ncheck:\n- your Java installation (try `java -version`)\n- your PySpark and Pandas installations (run `pip3 show pyspark pandas`)\n\nFor more information, see the Tumult Analytics installation instructions\nat\nhttps://docs.tmlt.dev/analytics/latest/howto-guides/installation.html"
     ]
    }
   ],
   "source": [
    "check_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e96a680-c8c4-443b-85d3-c1ce33a2a3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.16\" 2025-07-15\n",
      "OpenJDK Runtime Environment (build 17.0.16+8-Ubuntu-0ubuntu124.04.1)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.16+8-Ubuntu-0ubuntu124.04.1, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version # should be 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e716cc-891c-47b6-9702-89f799de6f67",
   "metadata": {},
   "source": [
    "not possible on onyxia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3aca5-6b4c-4789-910d-e2fbbf7c4341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
